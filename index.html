<!DOCTYPE HTML>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>Sanjay Jyoti Dutta</title>
<meta content="Sanjay Jyoti Dutta" name="author"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="images/sad64.png" rel="icon" type="image/png"/>
<style>
html, body {
    margin: 0;
    padding: 0;
    height: 100%;
    background-color: #f2f5f8; /* Lighter dark background */
    color: #393a31; /* GitHub dark dimmed text color */
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji";
}

/* Flexbox setup for the page */
body {
    display: flex;
    flex-direction: column;
    min-height: 100vh;
}

/* Content takes up all available space */
.content {
    flex: 1;
    padding: 20px;
}

/* Footer at the bottom */
footer {
    text-align: center;
    padding: 10px 0;
    background-color: #f2f5f8; /* Slightly darker for footer background */
    color: #393a31; /* Slightly lighter color for footer text for contrast */
    position: relative;
    width: 100%;
}

    /* Styling for buttons */
    .button {
      background-color: orange;
      color: white;
      padding: 5px 10px;
      text-decoration: none;
      border-radius: 3px;
      display: inline-block;
    }

    /* Styling for "Back to Top" button */
    .button-top {
      font-weight: bold;
    }

    /* Text justification */
    .justified-text p, .justified-text li {
      text-align: justify;
      text-justify: inter-word;
    }
  </style>
<style>
    .justified-text {
        text-align: justify;
        text-justify: inter-word;
    }
</style></head>
<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
<td style="padding:0px">
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
<td style="padding:2.5%;width:63%;vertical-align:middle">
<p class="justified-text" style="text-align:center">
<name>Sanjay Jyoti Dutta</name>
</p>
<p class="justified-text">I obtained my PhD in Computer Science, specializing in Computer Vision and Machine Learning under the supervision of <a href="https://www.aber.ac.uk/en/cs/staff-profiles/listing/profile/rrz/" target="_blank">Prof Reyer Zwiggelaar</a> and <a href="https://www.aber.ac.uk/en/cs/staff-profiles/listing/profile/tob45/" target="_blank">Prof Tossapon Boongoen</a> at <a href="https://www.aber.ac.uk/en/" target="_blank">Aberystwyth University</a>, United Kingdom. I am passionate about learning machine learning algorithms and solving practical challenges in their application to real-world problems.
              </p>
<p class="justified-text">
  My research focuses on the application of machine learning, particularly deep learning, for image and video analysis. In video analysis, I develop deep learning architectures for egocentric videos that integrate both action appearance and motion within a single model. The development and evaluation are conducted using publicly available datasets.

               </p>
<p class="justified-text"> 

                In addition, I had the opportunity to collaborate with the Life Sciences and Veterinary Sciences departments at my university on several machine learning projects. This interdisciplinary work involved developing and applying advanced machine learning techniques to solve complex problems in biological and veterinary sciences, enhancing research outcomes and fostering innovation at the intersection of technology and life sciences.

               </p>
<br/>
<!--<p>
                I have a master's degree in Computer Application from <a href="https://jecassam.ac.in/">Jorhat Engineering College</a>, Assam, India.
              </p>-->
<p class="justified-text" style="text-align:center">
<a href="mailto:sanjaydutta012@gmail.com">Email</a>  / 
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
<!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
<a href="https://scholar.google.co.in/citations?user=B7lUHrgAAAAJ&amp;hl=en">Google Scholar</a>  / 
                <a href="https://www.researchgate.net/profile/Sanjay-Dutta-4">Researchgate</a>  /  
                <a href="https://www.linkedin.com/in/sanjay-dutta/">Linkedin</a>  / 
                <a href="https://github.com/sanjay-dutta">Github</a>  / 
		            <a href="https://twitter.com/SanjaY__DuttA">X (Twitter)</a>  /  
                <a href="https://medium.com/@sanjay_dutta">Medium</a>  / <br/><br/>
<a class="underline" href="https://orcid.org/0009-0007-6149-789X" id="cy-effective-orcid-url" rel="me noopener noreferrer" style="vertical-align: top" target="orcid.widget">
<img alt="ORCID iD icon" src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width: 1em; margin-inline-start: 0.5em"/>
                  https://orcid.org/0009-0007-6149-789X
                </a>
<p class="justified-text" style="text-align:center">
<a class="button" href="#funProjects"><strong>Visit my fun projects</strong> </a>
</p>
</p>
</td>
<td style="padding:2.5%;width:40%;max-width:40%">
<!-- <a href="images/dip.png"><img alt="profile photo" class="hoverZoomLink" src="images/me.jpg" style="width:100%;max-width:100%"/></a> -->
<a href="images/dip1.png">
  <img 
    alt="profile photo" 
    class="hoverZoomLink" 
    src="images/dip1.png" 
    style="width:100%; max-width:100%; border-radius:50%;"
  />
</a>

</td>
</tr>
<!--</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Hobbies</heading>
            <p>
              My hobbies are playing guitar, singing and travelling, I also teach guitar. In addition, I was also part of a world record musical event in 2013 and hold a certification in Guitar from Rock School, London, UK. 
                          </p>
          </td>
        </tr> -->
<!--</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, deep learning, machine learning, image processing and video analysis. 
                            </p>
            </td>
          </tr> -->
</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tr>
<td style="padding:2.5%;width:100%;vertical-align:middle">
<h2>Research</h2>
<p class="justified-text"> I'm interested in Computer Vision, Deep Learning, Machine Learning, Image Processing and Video Analysis. </p>
</td>
</tr>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<!-- 
<tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
    <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
            <div class="two" id='ibrnet_image'><video width=100% height=100% muted autoplay loop>
            <source src="images/xyz.jpg" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/car.png' width="160">
        </div>
        <script type="text/javascript">
            function ibrnet_start() {
                document.getElementById('ibrnet_image').style.opacity = "1";
            }

            function ibrnet_stop() {
                document.getElementById('ibrnet_image').style.opacity = "0";
            }
            ibrnet_stop()
        </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
        Commenting out the entire block to hide from rendering but keep for future reference
        <a href="https://ibrnet.github.io/">
            <papertitle>Deep Learning Advances in Computer Vision: A Survey on use of Deep Learning Techniques for Autonomous Vehicles</papertitle>
        </a>
        <br>
        <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
        <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
        <a href="https://www.kylegenova.com/">Kyle Genova</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
        <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
        <strong>Jonathan T. Barron</strong>, 
        <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
        <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
        <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
        <br>
        <em>, On going</em>, 2023
        <br>
        <a href="Uploading">Publication</a> 
        <a href="https://ibrnet.github.io/">project page</a> /
        <a href="Uploading">code</a> /
        <a href="https://arxiv.org/abs/2102.13090">arXiv</a>
        <p></p>
        <p> Will be updated. </p>
        
    </td>
  </tr> 
  -->
<tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<div class="two" id="ibrnet_image"><video autoplay="" height="100%" loop="" muted="" width="100%">
<source src="images/xyz.jpg" type="video/mp4"/>
                Your browser does not support the video tag.
                </video></div>
<img src="images/human.jpg" width="160"/>
</div>
<script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('ibrnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('ibrnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
</td>


<td style="padding:20px;width:75%;vertical-align:middle">
  <!-- <a href="https://ibrnet.github.io/"> -->
  <papertitle>Human Activity Recognition</papertitle>
  
  <!--<br>
                <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
                <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
                <a href="https://www.kylegenova.com/">Kyle Genova</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
                <strong>Jonathan T. Barron</strong>, 
                <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
                <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
                <br>-->
  <em>, On going</em>, 2024
                <br/>
  <a href="Uploading">Publication</a>
  <!-- <a href="https://ibrnet.github.io/">project page</a> / -->
  <!-- <a href="Uploading">code</a> / -->
  <!-- <a href="https://arxiv.org/abs/2102.13090">arXiv</a>-->
  <p class="justified-text"></p>
  <p class="justified-text"> Will be updated. </p>
  </td>

<td style="padding:20px;width:75%;vertical-align:middle">
<!-- <a href="https://ibrnet.github.io/"> -->
<papertitle>Human Activity Recognition</papertitle>

<!--<br>
              <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
              <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
              <a href="https://www.kylegenova.com/">Kyle Genova</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
              <strong>Jonathan T. Barron</strong>, 
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
              <br>-->
<em>, On going</em>, 2024
              <br/>
<a href="Uploading">Publication</a>
<!-- <a href="https://ibrnet.github.io/">project page</a> / -->
<!-- <a href="Uploading">code</a> / -->
<!-- <a href="https://arxiv.org/abs/2102.13090">arXiv</a>-->
<p class="justified-text"></p>
<p class="justified-text"> Will be updated. </p>
</td>
</tr>
<tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<div class="two" id="ibrnet_image"><video autoplay="" height="100%" loop="" muted="" width="100%">
<source src="images/xyz.jpg" type="video/mp4"/>
                Your browser does not support the video tag.
                </video></div>
<img src="images/sheep.jpg" width="160"/>
</div>
<script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('ibrnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('ibrnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
<!-- <a href="https://ibrnet.github.io/"> -->
<papertitle>Sheep Lambing Detection</papertitle>

<!--<br>
              <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
              <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
              <a href="https://www.kylegenova.com/">Kyle Genova</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
              <strong>Jonathan T. Barron</strong>, 
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
              <br>-->
<em>, On going</em>, 2024
              <br/>
<a href="Uploading">Publication</a>
<!-- <a href="https://ibrnet.github.io/">project page</a> / -->
<!-- <a href="Uploading">code</a> / -->
<!-- <a href="https://arxiv.org/abs/2102.13090">arXiv</a>-->
<p class="justified-text"></p>
<p class="justified-text"> Will be updated. </p>
</td>
</tr>
<tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<div class="two" id="ibrnet_image"><video autoplay="" height="100%" loop="" muted="" width="100%">
<source src="images/noise.jpg" type="video/mp4"/>
                Your browser does not support the video tag.
                </video></div>
<img src="images/noise.jpg" width="160"/>
</div>
<script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('ibrnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('ibrnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
<!-- <a href="https://ibrnet.github.io/"> -->
<papertitle>Noise Profiling for ANNs: A Bio-inspired Approach</papertitle>

<!--<br>
              <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
              <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
              <a href="https://www.kylegenova.com/">Kyle Genova</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
              <strong>Jonathan T. Barron</strong>, 
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
              <br>-->
<em></em>, 2023
              <br/>
<a href="https://doi.org/10.1007/978-3-031-47508-5_12">Publication</a>
<!-- <a href="https://ibrnet.github.io/">project page</a> / -->
<!-- <a href="Uploading">code</a> / -->
<!-- <a href="https://doi.org/10.1007/978-3-031-47508-5_12">Springer</a> / -->
<p class="justified-text"></p>
<p class="justified-text">A novel approach to noise profiling for artificial neural networks (ANNs) is proposed, which is inspired by the sensory systems of insects. This approach entails the utilization of both Gaussian and Chaotic noises to enhance the adaptability, learning and generalization capabilities of ANNs.</p>
</td>
</tr>
<tr>
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<div class="two" id="mipnerf_image"><video autoplay="" height="100%" loop="" muted="" width="100%">
<!-- <source src="images/mipnerf_ipe_yellow.mp4" type="video/mp4"> -->
                Your browser does not support the video tag.
                </video></div>
<img src="images/vlab.jpg" width="160"/>
</div>
<script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }

                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
<!-- <a href="http://jonbarron.info/mipnerf"> -->
<papertitle>Virtual Lab phase II (Integration and maintenance)</papertitle>

<!-- <br> -->
<!-- <strong>Jonathan T. Barron</strong>, -->
<!-- <a href="https://bmild.github.io/">Ben Mildenhall</a>, -->
<!-- <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br> -->
<!-- <a href="https://phogzone.com/">Peter Hedman</a>, -->
<!-- <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>, -->
<!-- <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a> -->
<!-- <br> -->
<em>(2015-2017)</em>
<br/>
<a href="https://www.iitg.ac.in/cseweb/vlab/">project page</a>
              /
              <a href="https://ieeexplore.ieee.org/document/9033848">Publication</a>
<!-- <a href="https://youtu.be/EpH175PY1A0">video</a> -->
<!-- <a href="https://github.com/google/mipnerf">code</a> -->
<p class="justified-text"></p>
<p class="justified-text"><a href="https://www.iitg.ac.in/">Indian Institute of Technology Guwahati</a> is one of the contributors to Virtual laboratories, which are an essential part of E-learning because all the students in their institutes may not have sufficient lab facilities. These experiments can be accessed from anywhere and anytime. Therefore, the <a href="https://www.education.gov.in/hi"> Ministry of Human Resource Development (MHRD), Govt. of India</a> took an initiative of integration of virtual laboratories under the national mission on Education through Information and Communication Technology <a href="https://www.indiascienceandtechnology.gov.in/st-visions/national-mission/national-mission-education-through-ict-nmeict"> (NME-ICT) </a>. The motive of virtual lab integration is to make all the developed projects into an open source repository such that all the lab information is available to a community, students as well as academic institutes, for use and development, to convert all licensed contents into a platform that is independent of any licensed software.</p>
</td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<div class="two" id="nerfactor_image">
<!-- <img src='images/nerfactor_after.png' width="160"></div> -->
<img src="images/rtdsl.jpg" width="160"/>
</div>
<!-- <script type="text/javascript">
              function nerfactor_start() {
                document.getElementById('nerfactor_image').style.opacity = "1";
              }

              function nerfactor_stop() {
                document.getElementById('nerfactor_image').style.opacity = "0";
              }
              nerfactor_stop()
            </script> -->
</div></td>
<td style="padding:20px;width:75%;vertical-align:middle">
<!-- <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/"> -->
<papertitle>Remote Triggered Digital System Laboratory</papertitle>

<!-- <br> -->
<!-- <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>, -->
<!-- <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, -->
<!-- <a href="https://boyangdeng.com/">Boyang Deng</a>,<br> -->
<!-- <a href="https://www.pauldebevec.com/">Paul Debevec</a>, -->
<!-- <a href="http://billf.mit.edu/">William T. Freeman</a>, -->
<!-- <strong>Jonathan T. Barron</strong>, -->
<!-- <br> -->
<em>(2012-2015)</em>
<br/>
<a href="https://www.iitg.ac.in/cseweb/vlab/Digital-System-Lab/index.php">Project page</a>
            /
            <a href="https://ieeexplore.ieee.org/abstract/document/8529357">Publication</a>
<!-- <a href="https://www.youtube.com/watch?v=UUVSPJlwhPg">video</a> -->
<p class="justified-text"></p>
<p class="justified-text">We developed this project at <a href="https://www.iitg.ac.in/">Indian Institute of Technology Guwahati.</a> <a href="https://www.education.gov.in/hi">The Ministry of Human Resource Development (MHRD), Govt. of India</a> took the initiative of Remote Triggered Digital System Laboratory under the National Mission on Education through Information and Communication Technology <a href="https://www.indiascienceandtechnology.gov.in/st-visions/national-mission/national-mission-education-through-ict-nmeict"> (NME-ICT)</a>. This virtual laboratory provides the theoretical understanding of digital electronics to the students by performing various experiments.</p>
</td>
<tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<div class="two" id="nerfbake_image"><video autoplay="" height="100%" loop="" muted="" width="100%">
<!-- <source src="images/nerfbake_15.mp4" type="video/mp4"> -->
                Your browser does not support the video tag.
                </video></div>
<img src="images/hrms.jpg" width="160"/>
</div>
<script type="text/javascript">
                function nerfbake_start() {
                  document.getElementById('nerfbake_image').style.opacity = "1";
                }

                function nerfbake_stop() {
                  document.getElementById('nerfbake_image').style.opacity = "0";
                }
                nerfbake_stop()
              </script>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
<!-- <a href="http://nerf.live"> -->
<papertitle>Human Resource Management System </papertitle>
<!-- </a> -->
<!-- <br> -->
<!-- <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://www.pauldebevec.com/">Paul Debevec</a>
              <br> -->
<em></em>
<br/>
<a href="https://www.researchgate.net/publication/335741418_Human_Resource_Management_System_A_case_study_on_an_Information_Management_Design">Publication</a>
<!-- / -->
<!-- <a href="https://arxiv.org/abs/2103.14645">arXiv</a> -->
<!-- / -->
<!-- <a href="https://www.youtube.com/watch?v=5jKry8n5YO8">video</a> -->
<!-- / -->
<!-- <a href="https://nerf.live/#demos">demo</a> -->
<p class="justified-text"></p>
<p class="justified-text">The Human Management System is a <a href="https://www.java.com/en/">  Java based (J2EE) </a> system which provides intranet automation of HR software. The aim of the paper is based on a project that helps the overall management of the employees, who work in a company. The proposed system contains all the information regarding employees in the company. The system is developed on good interaction as well as communication facilities between the HR administrator and the working employees.</p>
</td>
</tr></tbody></table>
<table align="center" border="0" cellpadding="20" cellspacing="0" width="100%"><tbody>
<tr>
<td>
<heading>Attented Conferences and Workshops</heading>
</td>
</tr>
</tbody></table>
<table align="center" border="0" cellpadding="20" width="100%"><tbody>
<tr>
<!-- <td style="padding:20px;width:25%;vertical-align:middle"></td> -->
<td class="justified-text" valign="center" width="100%">
<ul>
<li class="justified-text">Postgraduate Research Conference 2024, Aberystwyth University, United Kingdom, April 2024.</li>
<li class="justified-text">The AI Research Hub Symposium, Aberystwyth University, United Kingdom, September 2023.</li>
<li class="justified-text">The 22nd UK Workshop on Computational Intelligence, Aston University, United Kingdom, September 2023.</li>
<li class="justified-text">Faculty of Business and Physical Sciences Postgraduate Research Conference 2023, Aberystwyth University, United Kingdom, July 2023.</li>
<li class="justified-text">1st AI Summer School for Beginners, Aberystwyth University, United Kingdom, August 2022.</li>
<li class="justified-text">5th Summer school on Artificial Intelligence, Indian Institute of Information Technology Hyderabad, India, August 2021.</li>
<li class="justified-text">IEEE 5th International conference for Convergence in technology, Pune, India, March 2019.</li>
<li class="justified-text">IEEE Fourteenth International Conference on Information Processing (IcInPro), Bangalore, December 2018.</li>
<li class="justified-text">IEEE 3rd International Conference for Convergence in Technology (I2CT), Pune, India, April 2018.</li>
<li class="justified-text">MATLAB workshop in Reflux 7.0, Indian Institute of Technology, Guwahati, India, 2019.</li>
<li class="justified-text">Workshop in Data Science in Financial technology, Research Conclave, Indian Institute of Technology, Guwahati, India, 2019.</li>
<li class="justified-text">Workshop in machine learning, Research Conclave, Indian Institute of Technology, Guwahati, India, 2019.</li>
<li class="justified-text">Poster presentation in Research Conclave, Indian Institute of Technology, Guwahati, India, 2017.</li>
<li class="justified-text">A one-day workshop on virtual laboratory jointly organized by Assam Engineering College and Indian Institute of Technology, Guwahati, India, February 2017.</li>
<li class="justified-text">A One-day workshop on virtual laboratory jointly organized by NIT Meghalaya, Shillong and Indian Institute of Technology, Guwahati, India, November 2016.</li>
<li class="justified-text">A One-day workshop virtual laboratory jointly organized by Central Institute of Technology, Kokrajhar and Indian Institute of Technology, Guwahati, India, August 2016.</li>
<li class="justified-text"> Virtual Labs Summer Sprint Integration workshop, Indian Institute of Technology, Guwahati, India, 2015.</li>
<li class="justified-text">First Integration workshop, International Institute of Information Technology Hyderabad, India, 2014.</li>
<li class="justified-text">Worked as a volunteer, 33rd Foundations of Software Technology and Theoretical Computer Science, Indian Institute of Technology, Guwahati, India, 2013.</li>
</ul>
<!-- Fun Projects Section -->
<div id="funProjects" style="padding:20px;">
<h2>Fun Projects</h2>
<p class="justified-text"> Following are a collection of practice projects, which sparks my interest in further explorations.</p>
<!-- Newest Project: Example Project 3 -->
<div class="project">


  <div class="project">
    <papertitle>Tomato Leaf Disease Detection</papertitle>
    <br/>
    <a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Tomato%20Leaf%20Detection">Github</a>
    <p class="justified-text">The Tomato Leaf Disease Detection dataset, available on Kaggle, comprises over 20,000 labeled images across 10 classes, facilitating the classification of various tomato leaf diseases. This extensive collection supports the development and evaluation of machine learning models aimed at accurately identifying and diagnosing tomato plant ailments, thereby contributing to advancements in agricultural disease management.</p>
    </div> 

  <div class="project">
    <papertitle>Video Classification using Vision Transformer (ViT)</papertitle>
    <br/>
    <a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/VIT_video_classification">Github</a>
    <p class="justified-text">This project aimed to classify videos using a Vision Transformer (ViT) model applied to the UCF50 dataset, which contained videos of 50 different action classes. The process began with extracting and preprocessing frames from videos, resizing, and normalizing them to create a consistent dataset. The extracted features and labels were saved as .npy files for future use in training. The Vision Transformer model was then designed with custom layers for tubelet embedding and positional encoding to handle the spatial-temporal information in the video frames.</p>
    </div> 

  <div class="project">
    <papertitle>Image Classification using Vision Transformer (ViT)</papertitle>
    <br/>
    <a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Vit_flower">Github</a>
    <p class="justified-text">The Vision Transformer (ViT) stands out as a groundbreaking architecture that redefines how we approach computer vision tasks. By leveraging the Transformer model using TensorFlow, ViT processes images as sequences of patches, offering a compelling alternative to traditional Convolutional Neural Networks (CNNs). We walked through the implementation of ViT for classifying flower images. </p>
    </div>

<div class="project">
   <papertitle> Human Action Recognition Using Detectron2 and LSTM</papertitle>
   <br/>
   <a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Human-Action-Recognition-Using-Detectron2-And-Lstm">Github</a>
   <p class="justified-text">The goal was to combine Detectron2 for pose estimation and LSTM for action classification and we have built a powerful human action recognition system. </p>
   </div>

<div class="project">
<papertitle> Training an SVM Regressor on the California Housing Dataset</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/SVM%20regressor">Github</a>
<p class="justified-text">This code tunes the hyperparameters for an SVM regressor using the California housing dataset and evaluates the model's performance using Root Mean Squared Error (RMSE). </p>
</div>
<div class="project">
<papertitle> Train &amp; Fine-Tune a Decision Tree on the Moons Dataset</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Decision%20Tree">Github</a>
<p class="justified-text">The objectiv was to train a Decision Tree on the moons dataset, fine-tune it using grid search, evaluate its performance, and visualize the resulting decision boundary. </p>
</div>
<div class="project">
<papertitle>Regression using a Decision Tree</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Decision%20Tree">Github</a>
<p class="justified-text">The goal was to perform regression using Decision Trees, visualize the dataset and the resulting tree structure, providing a clear understanding of the model's decision-making process.</p>
</div>
<div class="project">
<papertitle>Train and Visualize a Decision Tree</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Decision%20Tree">Github</a>
<p class="justified-text">The primary goal was to train and visualize a Decision Tree classifier using the Iris dataset. Decision Trees are intuitive and powerful models for classification and regression tasks. By visualizing the Decision Tree, you can gain insights into how the model makes decisions based on the features of the dataset.</p>
</div>
<div class="project">
<papertitle>Comparing LinearSVC, SVC and SGDClassifier</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Classifiers">Github</a>
<p class="justified-text">The goal was to generates a linearly separable dataset, trains a LinearSVC, SVC with a linear kernel and SGDClassifier, and plots their decision boundaries to see if they produce roughly the same model.</p>
</div>
<div class="project">
<papertitle>Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/BatchGD">Github</a>
<p class="justified-text">The goal was to implement Batch Gradient Descent with early stopping for Softmax Regression. The early stopping mechanism monitors the cross-entropy loss and stops training if the validation error does not improve for a specified number of epochs (patience).</p>
</div>
<div class="project">
<papertitle>Titanic Dataset with Machine Learning (Kaggle Challenge!)</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Titanic">Github</a>
<p class="justified-text">The Titanic dataset is a classic machine learning problem. It provides information about the passengers aboard the Titanic, and the goal is to predict whether a passenger survived or not based on various features such as age, gender, class, and more. This project is an excellent introduction to data cleaning, feature engineering, and model building. We prepared the data for training and then train a RandomForestClassifier.</p>
</div>
<div class="project">
<papertitle>Data Augmentation Using KNeighborsClassifier</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/MNIST">Github</a>
<p class="justified-text">In this project, we have done MNIST Classification with Data Augmentation Using KNeighborsClassifier</p>
</div>
<div class="project">
<papertitle>KNeighborsClassifier for MNIST</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/MNIST">Github</a>
<p class="justified-text">In this project, we built a machine learning classifier for the MNIST dataset, aiming to achieve over 97% accuracy on the test set. The MNIST dataset is a classic dataset in the field of machine learning, consisting of 70,000 images of handwritten digits (0–9). Each image is 28x28 pixels, and the task is to classify each image into the corresponding digit.</p>
</div>
<div class="project">
<papertitle>Football Player Segmentation with U-Net</papertitle>
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Football%20Player%20Segmentation_UNet">Github</a>
<p class="justified-text">In this project using TensorFlow library, we analyzed images using semantic image segmentation. Semantic segmentation's goal is to categorize each pixel in an image into a class or object. We analyzed football (or soccer) player positions on the field. There are 512 images in the set, together with JSON file containing image information.</p>
</div>
<div class="project">
<papertitle>Training a DCGAN in PyTorch</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/DCGAN-pytorch">Github</a>
<p class="justified-text">We worked on how to train DCGAN Model using PyTorch to generate images.</p>
</div>
<div class="project">
<papertitle>PyTorch: Transfer Learning and Image Classification</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Transfer-learning-pytorch">Github</a>
<p class="justified-text">Here, the goal was to perform transfer learning for image classification using the PyTorch deep learning library.</p>
</div>
<div class="project">
<papertitle>PyTorch object detection with pre-trained networks</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Pytorch-object-detection">Github</a>
<p class="justified-text">Here, we used PyTorch to detect objects in input images using seminal, state-of-the-art image classification networks, including Faster R-CNN with ResNet, Faster R-CNN with MobileNet, and RetinaNet. Also performed real-time object detection in video streams.</p>
</div>
<div class="project">
<papertitle>PyTorch image classification with pre-trained networks</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Pytorch-image-classification">Github</a>
<p class="justified-text">Here we used PyTorch to classify input images using seminal, state-of-the-art image classification networks, including VGG, Inception, DenseNet, and ResNet. </p>
</div>
<papertitle>Regression with CNNs</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/keras-regression-cnns">Github</a>
<p class="justified-text">In this project, the goal was to train a Convolutional Neural Network (CNN) for regression prediction with Keras and then train a CNN to predict house prices from a set of images. </p>
</div>
<div class="project">
<papertitle>PyTorch: Training your first Convolutional Neural Network (CNN)</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Pytorch-cnn">Github</a>
<p class="justified-text">Here, A Convolutional Neural Network (CNN) is developed using the PyTorch deep learning library. This network will be able to recognize handwritten Hiragana characters. </p>
</div>
<div class="project">
<papertitle>Fashion MNIST with Keras and Deep Learning</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/fashion-mnist">Github</a>
<p class="justified-text">The objective of the project was to create a deep learning model to classify images of clothing from the Fashion MNIST dataset. The Fashion MNIST dataset is a collection of grayscale images of 10 different categories of clothing and accessories, like T-shirts, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots. </p>
</div>
<div class="project">
<papertitle>Smile detection with OpenCV, Keras, and TensorFlow</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/smile-detection">Github</a>
<p class="justified-text">This project used Haar cascade face detector, extract the face region of interest (ROI) from the image and then pass the ROI through LeNet for smile detection.</p>
</div>
<div class="project">
<papertitle>Breaking captchas with deep learning, Keras, and TensorFlow</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/breaking-captchas">Github</a>
<p class="justified-text">This project demonstrated how to use deep learning techniques, specifically with frameworks like Keras and TensorFlow, to automatically solve CAPTCHA challenges.</p>
</div>
<div class="project">
<papertitle>Use Checkpoint Strategies with Keras and TensorFlow</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Model-checkpoint-tensorflow">Github</a>
<p class="justified-text">This amied to use Early Stopping and Model Checkpointing in training Keras models encapsulates a sophisticated approach to deep learning.</p>
</div>
<div class="project">
<papertitle>ImageNet: VGGNet, ResNet, Inception, and Xception with Keras</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Keras-networks/keras-networks#imagenet-vggnet-resnet-inception-and-xception-with-keras">Github</a>
<p class="justified-text">The project was designed to classify an image by identifying the main subject in the image, leveraging pre-trained deep learning models available through TensorFlow's Keras library. It accepts an image file and a model name as input parameters. The script supports various state-of-the-art image classification models like VGG16, VGG19, ResNet50, InceptionV3, and Xception, which have been trained on the ImageNet dataset.</p>
</div>
<div class="project">
<papertitle>Visualize network architecture</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/keras-visualize-architecture">Github</a>
<p class="justified-text">The goal was to visualize network architecture using Keras and TensorFlow.</p>
</div>
<div class="project">
<papertitle>MiniVGGNet Implementation </papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Mini_vggnet">Github</a>
<p class="justified-text">The aim was to implement MiniVGGNet to work on CIFAR-10 data set.</p>
</div>
<div class="project">
<papertitle>LeNet: Recognizing Handwritten Digits </papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Keras_lenet">Github</a>
<p class="justified-text">The goal was for building, training, evaluating, and plotting the performance of a convolutional neural network (LeNet) for digit classification on the MNIST dataset.</p>
</div>
<div class="project">
<papertitle>First Deep Learning Project in Python </papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/First_CNN">Github</a>
<p class="justified-text">The goal was to create the  first deep learning neural network model in Python using Keras. Here, we started by loading and preparing our dataset, followed by defining and compiling a Keras neural network model. We trained the model on our data, evaluate its performance, and then use it to make predictions on new data. We used Pima Indians onset of diabetes dataset.</p>
</div>
<div class="project">
<papertitle>Implementing Convolutions with Python</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/convolutions-cross-correlation">Github</a>
<p class="justified-text">We explored hands-on code that illustrates how to implement and apply convolution operations and kernels to images. This insight aided in understanding the internal workings of Convolutional Neural Networks (CNNs) during their training phase.</p>
</div>
<div class="project">
<papertitle>Backpropagation from Scratch with Python</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Backpropagation">Github</a>
<p class="justified-text">Mastering Backpropagation: A Step-by-Step Guide to Implementing it with Python </p>
</div>
<div class="project">
<papertitle>Perceptron Neural Network</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/perceptron-neural-network">Github</a>
<p class="justified-text">The project demonstrated how a perceptron model could learn bitwise operations through a basic machine learning process involving training with input features and corresponding labels, followed by testing to evaluate the model's predictions. </p>
</div>
<div class="project">
<papertitle>Pedestrian Detection with 4 Different Computer Vision Techniques</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Pedestrian%20Detection">Github</a>
<p class="justified-text">This project explores pedestrian detection using four different computer vision techniques. <br/>
      Method 1: Background Subtraction + Contour Extraction <br/>      
      Method 2: Haar Cascades (Viola-Jones Classifiers) <br/>     
      Method 3: Histogram of Oriented Gradients (HOG) and Support Vector Machine (SVM) <br/>    
      Method 4: Single Shot Detector (SSD) with MobileNet
      
  </p></div>
<div class="project">
<papertitle>Object Detection in a Video</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Object_detection">Github</a>
<p class="justified-text">This project showcased the implementation of Haar Cascade classifiers for object detection in video streams. Haar Cascades are a popular method for object detection due to their efficiency and effectiveness, particularly in detecting faces and other predefined objects. Using OpenCV, this project demonstrates how to apply Haar Cascades to real-time video data to identify and track objects. </p>
</div>
<div class="project">
<papertitle>Hand Gesture Recognition</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Hand%20Gesture%20detection">Github</a>
<p class="justified-text">This project focused on counting fingers in a real-time video using OpenCV. </p>
</div>
<div class="project">
<papertitle>Smile Detection</papertitle>
<!-- <em>On going, 2024</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Smile_detector">Github</a>
<p class="justified-text">The Smile Detection Project aimed at identifying smiles real-time video feeds using a facial landmark detector to accurately determine the presence of a smile.</p>
</div>
<!-- Previous Project: Example Project 2 -->
<div class="project">
<papertitle>Face Detection</papertitle>
<!-- <em>Completed, 2023</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/Computer-Vision-Practice/tree/main/Face%20detection">Github</a>
<p class="justified-text">The Face Detection project aimed to identify and locate human faces within a digital image utilizing Haar Cascades.</p>
</div>
<!-- Oldest Project: Example Project 1 -->
<div class="project">
<papertitle>OpenCV Basics</papertitle>
<!-- <em>Completed, 2022</em> -->
<br/>
<a href="https://github.com/sanjay-dutta/OpenCV-Basics">Github</a>
<p class="justified-text">These project series provides an essential overview of computer vision techniques using OpenCV. It begins with fundamental image operations—loading, displaying, and pixel manipulation—then advances to drawing, translation, rotation, resizing, flipping, and cropping. Additionally, it explores arithmetic operations, bitwise manipulations, masking, and channel manipulation. Accompanied by downloadable source code for each tutorial, this series offers a practical and efficient way to grasp the key functionalities of OpenCV, making it perfect for beginners eager to learn quickly.</p>
</div>
<!-- Button to navigate back to the top of the page -->
<p class="justified-text" style="text-align:center">
<a class="button button-top" href="#top">Back to Top</a>
</p>
</div>
<!-- <div style="text-align:center; margin-top:20px;">
              <a href="#" class="btn"><strong>Back to Top</strong></a>
            </div> -->


</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody></table>
<footer>
  <p style="text-align: center; margin: 0;">© <span id="year"></span> Sanjay Jyoti Dutta</p>
</footer>
<script>
  document.getElementById('year').textContent = new Date().getFullYear();
</script>

</body>
</html>
